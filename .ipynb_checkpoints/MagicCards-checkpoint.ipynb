{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "import urllib.robotparser as robot\n",
    "from requests.exceptions import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dominio = ['https://www.capefeargames.com/','https://www.mtgotraders.com/',\n",
    "           'https://www.wizardscupboard.com/','https://abugames.com/',\n",
    "           'https://www.cardkingdom.com/','http://www.starcitygames.com/',\n",
    "           'https://www.tcgplayer.com/','https://scryfall.com/']\n",
    "\n",
    "paginasVisitadas = []\n",
    "paginas0 = []\n",
    "paginas1 = []\n",
    "paginas2 = []\n",
    "paginas3 = []\n",
    "paginas4 = []\n",
    "paginas5 = []\n",
    "paginas6 = []\n",
    "paginas7 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para marcar a page como visitada.\n",
    "def PaginaVisitda (url):\n",
    "    if url not in paginasVisitadas:\n",
    "        #paginasVisitdas.append(url)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar se a requisição sofreu algum problema.\n",
    "def verificarRequest(ListaUrls):\n",
    "    for url in ListaUrls:\n",
    "        try:\n",
    "            response = rq.get(url)\n",
    "            # Se o response retornou cod = 200 (sucesso), não teremos nenhuma exceção\n",
    "            response.raise_for_status()\n",
    "            \n",
    "        except HTTPError as http_err:\n",
    "            #print(f'HTTP error occurred: {http_err}')\n",
    "            return False\n",
    "        except Exception as err:\n",
    "            #print(f'Other error occurred: {err}')\n",
    "            return False\n",
    "        else:\n",
    "            #print('Success!')\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Verificar se podemos baixar a page ou não\n",
    "# Nota:Dominio(3 ,4 ,5) retornaram false.\n",
    "def verificarRobotTxt(url):\n",
    "    \n",
    "    robotUrl = robot.RobotFileParser()\n",
    "    robotUrl.set_url(url + \"/robots.txt\")\n",
    "    robotUrl.read()\n",
    "    if robotUrl.can_fetch('*',url) == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# dos sites do Dominio apena o Dominio[2] tem 15 de delay\n",
    "url = Dominio[2]\n",
    "robotUrl = robot.RobotFileParser()\n",
    "robotUrl.set_url(url + \"/robots.txt\")\n",
    "robotUrl.read()\n",
    "print(robotUrl.crawl_delay('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CardsCrawler(url):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # verificar se a requisição funciana.\n",
    "    if verificarRequest([url]) == True:\n",
    "        \n",
    "        #request - verificando se a page é html pelo content-type.\n",
    "        codigo_fonte = rq.get(url,headers={'content-type': 'text/html'})\n",
    "        \n",
    "        # texto,links dentro do codigo_fonte.\n",
    "        texto = codigo_fonte.text\n",
    "        soup = BeautifulSoup(texto,'html.parser')\n",
    "        \n",
    "        # Laço para encontrar todos os links da pagina(url).\n",
    "        for link in soup.findAll('a'):\n",
    "            if url == Dominio[count]:\n",
    "                href = link.get('href')\n",
    "                casos = {\n",
    "                    0:paginas0,\n",
    "                    1:paginas1,\n",
    "                    2:paginas2,\n",
    "                    3:paginas3,\n",
    "                    4:paginas4,\n",
    "                    5:paginas5,\n",
    "                    6:paginas6,\n",
    "                    7:paginas7\n",
    "                }\n",
    "                \n",
    "                casos[count].append(href)\n",
    "        \n",
    "        count =+1\n",
    "            \n",
    "        # Verifico se a pagina foi visitada, se não , add ela a lista de paginasVisitadas e retiro ela da lista Dominio.\n",
    "        if PaginaVisitda(url) == False:\n",
    "            paginasVisitadas.append(url)\n",
    "            #List(filter(lambda a: a != url, Dominio))\n",
    "            \n",
    "            while url in Dominio:\n",
    "                Dominio.remove(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in Dominio:\n",
    "    CardsCrawler(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paginas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['https://www.capefeargames.com/', 'https://www.wizardscupboard.com/', 'https://www.cardkingdom.com/', 'https://www.tcgplayer.com/', 'https://www.mtgotraders.com/', 'http://www.starcitygames.com/', 'https://abugames.com/', 'https://scryfall.com/']\n"
     ]
    }
   ],
   "source": [
    "print(Dominio)\n",
    "print(paginasVisitadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não tem href?\n",
      "Não tem href?\n"
     ]
    }
   ],
   "source": [
    "# Crawler não ta pegando nenhum link dos sites Dominio(3,6)\n",
    "CardsCrawler(Dominio[3]) \n",
    "print('Não tem href?')\n",
    "\n",
    "CardsCrawler(Dominio[6]) \n",
    "print('Não tem href?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Heurística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
