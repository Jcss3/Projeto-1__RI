{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: reppy in c:\\anaconda3\\lib\\site-packages (0.4.14)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from reppy) (1.12.0)\n",
      "Requirement already satisfied: cachetools in c:\\anaconda3\\lib\\site-packages (from reppy) (3.1.1)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from reppy) (2.21.0)\n",
      "Requirement already satisfied: python-dateutil!=2.0,>=1.5 in c:\\anaconda3\\lib\\site-packages (from reppy) (2.8.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->reppy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->reppy) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->reppy) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->reppy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install reppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "#import urllib.robotparser as robot\n",
    "from requests.exceptions import HTTPError\n",
    "from reppy.robots import Robots\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication\n",
    "from PyQt5.QtCore import QUrl, QEventLoop\n",
    "from PyQt5.QtWebEngineWidgets import QWebEnginePage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from reppy.robots import Robots\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dominio = ['https://www.capefeargames.com/','https://www.mtgotraders.com/',\n",
    "           'https://www.wizardscupboard.com/','https://abugames.com/',\n",
    "           'https://www.cardkingdom.com/','http://www.starcitygames.com/',\n",
    "           'https://www.tcgplayer.com/','https://scryfall.com/']\n",
    "\n",
    "paginasVisitadas = []\n",
    "\n",
    "paginasRelevante = {'Relevate':[],'Nao-Relevate':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para marcar a page como visitada.\n",
    "def Visitou (url):\n",
    "    if url not in paginasVisitadas:\n",
    "        #paginasVisitdas.append(url)\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar se a requisição sofreu algum problema.\n",
    "def verificarRequest(url):\n",
    "    try:\n",
    "        response = rq.head(url)\n",
    "        # Se o response retornou cod = 200 (sucesso), não teremos nenhuma exceção\n",
    "        response.raise_for_status()\n",
    "\n",
    "    except HTTPError as http_err:\n",
    "        #print(f'HTTP error occurred: {http_err}')\n",
    "        return False\n",
    "    except Exception as err:\n",
    "        #print(f'Other error occurred: {err}')\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlerDinâmico(url):\n",
    "    driver = webdriver.Chrome(\"C:\\\\Users\\\\jcss3\\\\Downloads\\\\chromedriver_win32 _76\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    driver.quit\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para Verificar se podemos baixar a page ou não\n",
    "def verificarRobotTxt(url):\n",
    "    \n",
    "    # This utility uses `requests` to fetch the content\n",
    "    robots = Robots.fetch(url + '/robots.txt')\n",
    "    return robots.allowed(url, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verificarRobotTxt(Dominio[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CardsCrawler(url):\n",
    "    print('Dominio: ' + url.geturl())\n",
    "    count = 0\n",
    "    paginas = [url]\n",
    "    #paginasVisitadas = []\n",
    "    robots = Robots.fetch(url.geturl() + '/robots.txt')\n",
    "    while count < 1000 and len(paginas) > 0:\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "        primeiraPagina = paginas.pop(0)\n",
    "        print(primeiraPagina.geturl())\n",
    "       \n",
    "        # Verifica se a pagina foi visitada.Se foi, pego o proximo link da lista.(continua até achar um que não foi visitado!)\n",
    "        while Visitou(primeiraPagina) == True or primeiraPagina.hostname != url.hostname or primeiraPagina.hostname == None:\n",
    "            if len(paginas) == 0:\n",
    "                print('lascou@!')\n",
    "                return\n",
    "            primeiraPagina = paginas.pop(0)\n",
    "        \n",
    "        if robots.allowed(primeiraPagina.geturl(), '*') == True:\n",
    "            # verificar se a requisição funciana.\n",
    "            if verificarRequest(primeiraPagina.geturl()) == True:\n",
    "\n",
    "                # Verficar se o site é dinamico\n",
    "                if (Dominio[3] in url) or (Dominio[6] in url):\n",
    "                    texto = CrawlerDinâmico(primeiraPagina.geturl())\n",
    "                else:\n",
    "                    #request - verificando se a page é html pelo content-type.\n",
    "                    codigo_fonte = rq.get(primeiraPagina.geturl(),headers={'content-type': 'text/html'})\n",
    "                    # texto,links dentro do codigo_fonte.\n",
    "                    texto = codigo_fonte.text\n",
    "                soup = BeautifulSoup(texto,'html.parser')\n",
    "\n",
    "                # Laço para encontrar todos os links da pagina(url). # attrs={'href': re.compile(\"^http://\")}\n",
    "                for link in soup.findAll('a'):\n",
    "                    href = link.get('href')\n",
    "                    href = urllib.parse.urljoin(url.geturl(),href)\n",
    "                    paginas.append(urlparse(href))\n",
    "\n",
    "                #marco como visitada a url    \n",
    "                paginasVisitadas.append(primeiraPagina)\n",
    "\n",
    "            else:\n",
    "                print('Requisição falhou!')\n",
    "        else:\n",
    "            print('Respeitando o Robot.txt não iremos ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominio: https://www.capefeargames.com/\n",
      "1\n",
      "https://www.capefeargames.com/\n",
      "lascou@!\n",
      "Dominio: https://www.mtgotraders.com/\n",
      "1\n",
      "https://www.mtgotraders.com/\n",
      "lascou@!\n",
      "Dominio: https://www.wizardscupboard.com/\n",
      "1\n",
      "https://www.wizardscupboard.com/\n",
      "2\n",
      "https://www.wizardscupboard.com\n",
      "3\n",
      "https://www.wizardscupboard.com/index.php\n",
      "4\n",
      "https://www.wizardscupboard.com/account.php\n",
      "Respeitando o Robot.txt não iremos \n",
      "5\n",
      "https://www.wizardscupboard.com/shopping_cart.php\n",
      "Respeitando o Robot.txt não iremos \n",
      "6\n",
      "https://www.wizardscupboard.com/checkout_shipping.php\n",
      "Respeitando o Robot.txt não iremos \n",
      "7\n",
      "https://www.wizardscupboard.com/advanced_search.php\n",
      "8\n",
      "https://www.wizardscupboard.com/singles-c-100.html\n",
      "9\n",
      "https://www.wizardscupboard.com/foils-c-1100.html\n",
      "10\n",
      "https://www.wizardscupboard.com/sets-c-500.html\n",
      "11\n",
      "https://www.wizardscupboard.com/booster-boxes-c-210.html\n",
      "12\n",
      "https://www.wizardscupboard.com/booster-packs-c-220.html\n",
      "13\n",
      "https://www.wizardscupboard.com/other-sealed-product-c-230.html\n"
     ]
    }
   ],
   "source": [
    "for url in Dominio:\n",
    "    CardsCrawler(urlparse(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlparse('mailto:info@capefeargames.com').hostname == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Heurística"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
